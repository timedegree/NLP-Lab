{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmm_word_seg(sentence, lexicon, max_word_len):\n",
    "    begin = 0\n",
    "    end = min(begin + max_word_len, len(sentence))\n",
    "    words = []\n",
    "    while begin < end:\n",
    "        word = sentence[begin:end]\n",
    "        if word in lexicon or end - begin == 1:\n",
    "            words.append(word)\n",
    "            begin = end\n",
    "            end = min(begin + max_word_len, len(sentence))\n",
    "        else:\n",
    "            end -= 1\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def rmm_word_seg(sentence, lexicon, max_word_len):\n",
    "    begin = len(sentence)\n",
    "    end = max(begin - max_word_len, 0)\n",
    "    words = []\n",
    "    while begin > end:\n",
    "        word = sentence[end:begin]\n",
    "        if word in lexicon or begin - end == 1:\n",
    "            words.append(word)\n",
    "            begin = end\n",
    "            end = max(begin - max_word_len, 0)\n",
    "        else:\n",
    "            end += 1\n",
    "\n",
    "    return words[::-1]\n",
    "\n",
    "\n",
    "def bmm_word_seg(sentence, lexicon, max_word_len):\n",
    "    fmm_words = fmm_word_seg(sentence, lexicon, max_word_len)\n",
    "    rmm_words = rmm_word_seg(sentence, lexicon, max_word_len)\n",
    "\n",
    "    if fmm_words == rmm_words:\n",
    "        return fmm_words\n",
    "\n",
    "    if len(fmm_words) < len(rmm_words):\n",
    "        return fmm_words\n",
    "    elif len(fmm_words) > len(rmm_words):\n",
    "        return rmm_words\n",
    "\n",
    "    fmm_single_words = 0\n",
    "    rmm_single_words = 0\n",
    "    for i in fmm_words:\n",
    "        if len(i) == 1:\n",
    "            fmm_single_words += 1\n",
    "    for i in rmm_words:\n",
    "        if len(i) == 1:\n",
    "            rmm_single_words += 1\n",
    "\n",
    "    if fmm_single_words < rmm_single_words:\n",
    "        return fmm_words\n",
    "    elif fmm_single_words > rmm_single_words:\n",
    "        return rmm_words\n",
    "    \n",
    "    fmm_not_lexicon_words = 0\n",
    "    rmm_not_lexicon_words = 0\n",
    "\n",
    "    for i in fmm_words:\n",
    "        if i not in lexicon:\n",
    "            fmm_not_lexicon_words += 1\n",
    "\n",
    "    for i in rmm_words:\n",
    "        if i not in lexicon:\n",
    "            rmm_not_lexicon_words += 1\n",
    "\n",
    "    if fmm_not_lexicon_words < rmm_not_lexicon_words:\n",
    "        return fmm_words\n",
    "    else:\n",
    "        return rmm_words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(filename):\n",
    "    lexicon = []\n",
    "    words_count = []\n",
    "    max_word_len = 0\n",
    "\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            lexicon.append(line.strip().split()[0])\n",
    "            words_count.append(int(line.strip().split()[1]))\n",
    "            if len(line.strip()) > max_word_len:\n",
    "                max_word_len = len(line.strip())\n",
    "    \n",
    "    return lexicon, words_count, max_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon, words_count, max_word_len = load_lexicon('./dict.txt.big')\n",
    "\n",
    "sentence = '南京市长江大桥'\n",
    "print(fmm_word_seg(sentence, lexicon, max_word_len))\n",
    "print(rmm_word_seg(sentence, lexicon, max_word_len))\n",
    "print(dmm_word_seg(sentence, lexicon, max_word_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jieba.finalseg.prob_start import P as prob_start\n",
    "from jieba.finalseg.prob_trans import P as prob_trans\n",
    "#from jieba.finalseg.prob_emit import P as prob_emit\n",
    "\n",
    "\n",
    "print(prob_start, prob_trans, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_emit = {label:Counter() for label in \"SBME\"}\n",
    "for i in range(len(words_count)):\n",
    "    if len(lexicon[i]) == 1:\n",
    "        count_emit['S'][lexicon[i]] += words_count[i]\n",
    "    else:\n",
    "        count_emit['B'][lexicon[i][0]] += words_count[i]\n",
    "        count_emit['E'][lexicon[i][-1]] += words_count[i]\n",
    "        for char in lexicon[i][1:-1]:\n",
    "            count_emit['M'][char] += words_count[i]\n",
    "log_total = {label:log(sum(count_emit[label].values())) for label in \"SBME\"}\n",
    "prob_emit = {label:{char:log(count_emit[label][char]) - log_total[label] for char in count_emit[label]} for label in \"SBME\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FLOAT = -3.14e100\n",
    "PrevStatus = {\n",
    "    'B': 'ES',\n",
    "    'M': 'MB',\n",
    "    'S': 'SE',\n",
    "    'E': 'BS'\n",
    "}\n",
    "\n",
    "def viterbi(sentence, states, prob_start, prob_trans, prob_emit):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    for state in states:\n",
    "        V[0][state] = prob_start[state] + prob_emit[state].get(sentence[0], MIN_FLOAT)\n",
    "        path[state] = [state]\n",
    "\n",
    "    for i in range(1,len(sentence)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    "\n",
    "        for state in states:\n",
    "            prob, _state = max([V[i-1][st] + prob_trans[st].get(state, MIN_FLOAT) + prob_emit[state].get(sentence[i], MIN_FLOAT), st] for st in PrevStatus[state])\n",
    "            V[i][state] = prob\n",
    "            newpath[state] = path[_state] + [state]\n",
    "\n",
    "        path = newpath\n",
    "\n",
    "    prob, state = max((V[len(sentence) - 1][state], state) for state in 'ES')\n",
    "    return prob, path[state]\n",
    "\n",
    "def HMM_seg(sentence):\n",
    "    prob, state = viterbi(sentence, 'SBME', prob_start, prob_trans, prob_emit)\n",
    "\n",
    "    print(prob, state)\n",
    "\n",
    "    words = []\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        if state[i] in 'BS':\n",
    "            words.append(sentence[i])\n",
    "        else:\n",
    "            words[-1] += sentence[i]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '单字词的概率好大'\n",
    "\n",
    "print(HMM_seg(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
